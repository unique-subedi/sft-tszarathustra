output_dir: out/booksft-sft
max_seq_len: 2048
per_device_train_batch_size: 1
gradient_accumulation_steps: 32
num_train_epochs: 2
learning_rate: 2.0e-4
warmup_ratio: 0.05
eval_steps: 200
save_steps: 500
lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_up_down"]
